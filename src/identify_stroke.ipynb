{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(prev_hn) = 27735\n"
     ]
    }
   ],
   "source": [
    "from folder import StandardFolder\n",
    "from polars_utils import *\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "\n",
    "prev_hn = pl.read_csv('D:/Prut/Warehouses/output/Dec23/n/Stroke/stroke_n_updatedSep2023_28112023.csv').to_series().to_list()\n",
    "print(f'{len(prev_hn) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokeIdentify(StandardFolder):\n",
    "    def __init__(self, folder: str, streaming: bool = True) -> None:\n",
    "        super().__init__(folder)\n",
    "        self.streaming = streaming\n",
    "        self.export_folder = Path('../output/Dec23/wh/complete')\n",
    "        self.select_dx = [f'I6{i}' for i in range(10)] + ['G45']\n",
    "        self.select_dx_re = '^' + '|^'.join(self.select_dx)\n",
    "        # self.prev_hn = pl.read_csv('D:/Prut/Warehouses/output/Dec23/n/Stroke/stroke_n_updatedSep2023_28112023.csv').to_series().to_list()\n",
    "        self.ran_all = False\n",
    "\n",
    "    def get_dx(self, select: list = None):\n",
    "        folder_path = self.dx\n",
    "        to_concat = []\n",
    "        for path in folder_path.iterdir():\n",
    "            file = (\n",
    "                scan_file(path)\n",
    "                .select(pl.col(['ENC_HN', 'D001KEY', 'D035KEY']))\n",
    "                .pipe(parse_dates, 'D001KEY')\n",
    "            )\n",
    "            if select is not None:\n",
    "                file = file.filter(pl.col('D035KEY').str.contains(self.select_dx_re))\n",
    "            file = file.group_by(pl.col(['ENC_HN', 'D001KEY'])).agg(pl.col('D035KEY')).with_columns(pl.col('D035KEY').list.unique().list.sort().list.join(', '))\n",
    "            to_concat.append(file.collect(streaming=self.streaming))\n",
    "        self.dx_df = pl.concat(to_concat).unique()\n",
    "\n",
    "\n",
    "    def get_demo(self):\n",
    "        folder_path = self.demo\n",
    "        cols = ['ENC_HN', 'D020AT3', 'H2L1KEY', 'H6L1KEY', 'H6L1DES']\n",
    "        new_col_names = ['ENC_HN', 'DOB', 'Sex', 'Province_ID', 'Province_Thai']\n",
    "        to_concat = []\n",
    "        for path in folder_path.iterdir():\n",
    "            file = scan_file(path)\n",
    "            if set(cols).issubset(set(file.columns)):\n",
    "                file = file.select(cols).collect(streaming=self.streaming).pipe(parse_dates, 'D020AT3') # New bug: only works in dataframes, so must collect first\n",
    "                to_concat.append(file)\n",
    "        self.demo_df = pl.concat(to_concat).unique()\n",
    "        self.demo_df = self.demo_df.rename(dict(zip(cols, new_col_names)))\n",
    "\n",
    "    def run_all(self):\n",
    "        self.get_dx(select=self.select_dx)\n",
    "        print('dx')\n",
    "        self.get_demo()\n",
    "        print('demo')\n",
    "        \n",
    "        self.ran_all = True\n",
    "\n",
    "    def merge(self):\n",
    "        if not self.ran_all:\n",
    "            raise Exception('Please run all first.')\n",
    "        \n",
    "        self.merged_df = (\n",
    "            self.dx_df\n",
    "            .join(self.demo_df, on=['ENC_HN'], how='left')\n",
    "            .unique()\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        # Remove previous\n",
    "            # .pipe(self.remove_previous_hn)\n",
    "            # .pipe(print_n)\n",
    "            # Clip dates\n",
    "            # .pipe(clip_dates, date_col='D001KEY', start_month=7, start_year=2023, end_month=12, end_year=2023)\n",
    "            # .pipe(print_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readme not included.\n",
      "dx\n",
      "demo\n"
     ]
    }
   ],
   "source": [
    "s = StrokeIdentify(folder='D:/Datalake/Data/20231231_fu_nc')\n",
    "s.run_all()\n",
    "s.merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to rules set by others on how to draw the flowchart, the following functions cannot be incorporated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_previous_hn(lf: pl.LazyFrame, prev_hn=prev_hn) -> pl.LazyFrame:\n",
    "    return lf.filter(~pl.col('ENC_HN').is_in(prev_hn))\n",
    "\n",
    "def print_n(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        print(df['ENC_HN'].n_unique())\n",
    "        return df\n",
    "\n",
    "def remove_previous_stroke(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Remove patients who are diagnosed with stroke before entering the cohort\n",
    "    return df.sort('D001KEY').group_by('ENC_HN', maintain_order=True).first().filter(~pl.col('D035KEY').map_elements(lambda x: 'I69' in x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2788\n",
      "1095\n",
      "1075\n",
      "28810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC5\\AppData\\Local\\Temp\\ipykernel_665784\\1560725653.py:10: PolarsInefficientMapWarning: \n",
      "Expr.map_elements is significantly slower than the native expressions API.\n",
      "Only use if you absolutely CANNOT implement your logic otherwise.\n",
      "Replace this expression...\n",
      "  - pl.col(\"D035KEY\").map_elements(lambda x: ...)\n",
      "with this one instead:\n",
      "  + 'I69'.is_in(pl.col(\"D035KEY\"))\n",
      "\n",
      "  return df.sort('D001KEY').group_by('ENC_HN', maintain_order=True).first().filter(~pl.col('D035KEY').map_elements(lambda x: 'I69' in x))\n"
     ]
    }
   ],
   "source": [
    "# Flow box 1\n",
    "s.merged_df.pipe(clip_dates, date_col='D001KEY', start_month=7, start_year=2023, end_month=12, end_year=2023).pipe(remove_previous_hn).pipe(print_n)\n",
    "# Flow box 2\n",
    "s1 = s.merged_df.pipe(remove_previous_stroke).pipe(clip_dates, date_col='D001KEY', start_month=7, start_year=2023, end_month=12, end_year=2023)\n",
    "s1.pipe(remove_previous_hn).pipe(print_n)\n",
    "# Flow box 3\n",
    "s2 = s1.pipe(remove_previous_hn).filter((pl.col('D001KEY') - pl.col('DOB')) >= pl.duration(days=365*18))\n",
    "s2.pipe(print_n)\n",
    "# Save output\n",
    "prev = pl.read_csv('D:/Prut/Warehouses/output/Dec23/n/Stroke/stroke_n_updatedSep2023_28112023.csv').pipe(parse_dates, 'Date')\n",
    "new = s2[['ENC_HN', 'D001KEY', 'D035KEY']].rename({'D001KEY': 'Date', 'D035KEY': 'ICD10'})\n",
    "output = pl.concat([new, prev])\n",
    "output.pipe(print_n)\n",
    "\n",
    "output.write_csv('D:/Prut/Warehouses/output/Dec23/n/Stroke/stroke_n_updated_05032024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
