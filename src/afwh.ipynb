{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23499\n"
     ]
    }
   ],
   "source": [
    "from polars_utils import *\n",
    "from folder import StandardFolder\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "hn = pl.read_csv('D:/Prut/Warehouses/output/Dec23/n/AF/AF_201001_202306.csv').to_series().to_list() + \\\n",
    "    pl.read_csv('D:/Prut/Warehouses/output/Dec23/n/AF/af_for_aj_eak_180124.csv').to_series().to_list()\n",
    "\n",
    "print(len(hn)) # 23499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFWarehouse(StandardFolder):\n",
    "    def __init__(self, hn_list: list[str], folder: str, streaming: bool = True) -> None:\n",
    "        super().__init__(folder)\n",
    "        self.hn_list = hn_list\n",
    "        self.streaming = streaming\n",
    "        self.lab_conversion = pl.read_csv('../std/lab_conversion.csv')\n",
    "        self.meds_to_select = pl.read_csv('../std/meds_AF.csv')\n",
    "        self.export_folder = Path('../output/Dec23/wh/complete')\n",
    "        self.ran_all = False\n",
    "\n",
    "    def get_labs(self, selected_labs: list[str] = ['Glucose', 'Creatinine', 'LDL', 'HDL', 'Triglyceride']):\n",
    "        folder_path = self.lab\n",
    "        to_concat = []\n",
    "        for path in list(folder_path.iterdir()):\n",
    "            file = (\n",
    "                scan_file(path)\n",
    "                .select(pl.col(['ENC_HN', 'REPORT_DATE', 'SHORT_TEST', 'UNIT', 'RESULT_VAL']))\n",
    "                # Select HN\n",
    "                .filter(pl.col('ENC_HN').is_in(self.hn_list))\n",
    "                # Select wanted labs (on SHORT_TEST)\n",
    "                .pipe(identify_in_list, col_name='SHORT_TEST', criteria=selected_labs)\n",
    "                # Parse dates\n",
    "                .pipe(parse_dates, 'REPORT_DATE')\n",
    "                # Create a new column which is name+units\n",
    "                .with_columns(pl.concat_str('SHORT_TEST', 'UNIT', separator=',').alias('name_with_units'))\n",
    "                .filter(pl.col('name_with_units').is_in(self.lab_conversion.to_series()))\n",
    "                .join(self.lab_conversion.lazy(), how='left', on='name_with_units')\n",
    "                .rename({'new_name': 'Lab'})\n",
    "                # Clean lab values\n",
    "                .with_columns(pl.col('RESULT_VAL').str.extract(r'^\\d+\\.\\d+|\\d+$', 0).cast(pl.Float32))\n",
    "                .with_columns(pl.col('RESULT_VAL').str.replace(r'\\.{2,}', r'\\.'))\n",
    "                .with_columns(pl.col('RESULT_VAL').str.strip_chars('<>.,()/\\\\\\'\"'))\n",
    "                .filter(pl.col('RESULT_VAL').str.contains('^[0-9.]+$'))\n",
    "                # Multiply to standardise due to different units\n",
    "                .with_columns(pl.col('RESULT_VAL').mul(pl.col('mul_factor')))\n",
    "            )\n",
    "\n",
    "            # collect\n",
    "            file = file.collect(streaming=self.streaming)\n",
    "\n",
    "            # Pivot labs\n",
    "            file = file.pivot(values='RESULT_VAL', index=['ENC_HN', 'REPORT_DATE'], columns='Lab', aggregate_function='max')\n",
    "\n",
    "            # Rename a bit\n",
    "            file = file.rename({'REPORT_DATE': 'D001KEY'})\n",
    "\n",
    "            # Append\n",
    "            to_concat.append(file)\n",
    "\n",
    "        self.lab_df = pl.concat(to_concat, how='diagonal_relaxed').unique()\n",
    "        \n",
    "    def get_visits(self):\n",
    "        folder_path = self.visit\n",
    "        to_concat = []\n",
    "        for path in folder_path.iterdir():\n",
    "            file = (\n",
    "                scan_file(path)\n",
    "                .filter(pl.col('ENC_HN').is_in(self.hn_list))\n",
    "                .select(pl.col(['ENC_HN', 'D001KEY', 'D108KEY']))\n",
    "                .pipe(parse_dates, 'D001KEY')\n",
    "            )\n",
    "            to_concat.append(file.collect(streaming=self.streaming))\n",
    "        self.visit_df = pl.concat(to_concat).unique()\n",
    "\n",
    "    def get_deaths(self):\n",
    "        folder_path = self.deaths\n",
    "        to_concat = []\n",
    "        for path in folder_path.iterdir():\n",
    "            file = (\n",
    "                scan_file(path)\n",
    "                .filter(pl.col('ENC_HN').is_in(self.hn_list))\n",
    "                .select(pl.col('ENC_HN', 'D001KEY')).pipe(parse_dates, 'D001KEY').rename({'D001KEY': 'Death_date'})\n",
    "            )\n",
    "            to_concat.append(file.collect(streaming=self.streaming))\n",
    "        self.deaths_df = pl.concat(to_concat).unique()\n",
    "\n",
    "    def get_dx(self, select: list = None):\n",
    "        folder_path = self.dx\n",
    "        to_concat = []\n",
    "        for path in folder_path.iterdir():\n",
    "            file = (\n",
    "                scan_file(path)\n",
    "                .filter(pl.col('ENC_HN').is_in(self.hn_list))\n",
    "                .select(pl.col(['ENC_HN', 'D001KEY', 'D035KEY']))\n",
    "                .pipe(parse_dates, 'D001KEY')\n",
    "            )\n",
    "            if select is not None:\n",
    "                file = file.filter(pl.col('D035KEY').is_in(select))\n",
    "            file = file.group_by(pl.col(['ENC_HN', 'D001KEY'])).agg(pl.col('D035KEY')).with_columns(pl.col('D035KEY').list.unique().list.sort().list.join(', '))\n",
    "            to_concat.append(file.collect(streaming=self.streaming))\n",
    "        self.dx_df = pl.concat(to_concat).unique()\n",
    "\n",
    "    def get_meds(self):\n",
    "        folder_path = self.bill\n",
    "        select = self.meds_to_select.to_series().to_list()\n",
    "        to_concat = []\n",
    "        for path in folder_path.iterdir():\n",
    "            file = (\n",
    "                scan_file(path)\n",
    "                .filter(pl.col('ENC_HN').is_in(self.hn_list)))\n",
    "            if {'PER_DATE_2', 'SERVICE_ID', 'CAL_SER_AMT'}.issubset(file.columns):\n",
    "                file = file.rename({'PER_DATE_2': 'D001KEY', 'SERVICE_ID': 'D033KEY', 'CAL_SER_AMT': 'M1022'})\n",
    "            file = (\n",
    "                file\n",
    "                .select(pl.col(['ENC_HN', 'D001KEY', 'D033KEY', 'M1022']))\n",
    "                .pipe(parse_dates, 'D001KEY')\n",
    "            )\n",
    "            if select is not None:\n",
    "                file = file.filter(pl.col('D033KEY').is_in(select))\n",
    "\n",
    "            to_concat.append(file.collect(streaming=self.streaming))\n",
    "        \n",
    "        # pivot\n",
    "        self.meds_df = pl.concat(to_concat).pivot(index=['ENC_HN', 'D001KEY'], values='M1022', columns='D033KEY', aggregate_function='max').unique()\n",
    "\n",
    "        # rename\n",
    "        for k, v in zip(self.meds_to_select['CODE'], self.meds_to_select['dosed_name']):\n",
    "            if k in self.meds_df.columns:\n",
    "                self.meds_df = self.meds_df.rename({k:v})\n",
    "\n",
    "\n",
    "    def get_vs(self):\n",
    "        folder_path = self.vs\n",
    "        rename_long = {'RECORD': 'D001KEY', 'DIA': 'DBP', 'SYS': 'SBP', 'HRBP': 'HR'}\n",
    "        rename_wide = {'RECORDDATE': 'D001KEY', 'HIGH': 'HEIGHT', 'BW': 'WEIGHT'}\n",
    "        select = ['ENC_HN', 'D001KEY', 'SBP', 'DBP', 'HR', 'WEIGHT', 'HEIGHT', 'BMI']\n",
    "\n",
    "        to_concat = []\n",
    "        for path in folder_path.iterdir():\n",
    "            file = (\n",
    "                scan_file(path)\n",
    "                .filter(pl.col('ENC_HN').is_in(self.hn_list)))\n",
    "\n",
    "            # Must collect before pivot\n",
    "            file = file.collect(streaming=self.streaming)\n",
    "\n",
    "            # There are two formats, wide and long\n",
    "            if len(file.columns) < 10:\n",
    "                file = file.pivot(values = 'TEST_VALUE', index = ['ENC_HN', 'RECORD'], columns='TEST_NAME')\n",
    "                file = file.rename(rename_long)\n",
    "            else:\n",
    "                file = file.rename(rename_wide)\n",
    "            file = file.select(select).pipe(parse_dates, 'D001KEY')\n",
    "            to_concat.append(file)\n",
    "        \n",
    "        self.vs_df = pl.concat(to_concat).unique()\n",
    "\n",
    "\n",
    "    def get_demo(self):\n",
    "        folder_path = self.demo\n",
    "        cols = ['ENC_HN', 'D020AT3', 'H2L1KEY', 'H6L1KEY', 'H6L1DES']\n",
    "        new_col_names = ['ENC_HN', 'DOB', 'Sex', 'Province_ID', 'Province_Thai']\n",
    "        to_concat = []\n",
    "        for path in folder_path.iterdir():\n",
    "            file = scan_file(path)\n",
    "            if set(cols).issubset(set(file.columns)):\n",
    "                file = file.select(cols).collect(streaming=self.streaming).pipe(parse_dates, 'D020AT3') # New bug: only works in dataframes, so must collect first\n",
    "                to_concat.append(file)\n",
    "        self.demo_df = pl.concat(to_concat).unique()\n",
    "        self.demo_df = self.demo_df.rename(dict(zip(cols, new_col_names)))\n",
    "\n",
    "\n",
    "    def run_all(self):\n",
    "        print('start')\n",
    "        self.get_meds()\n",
    "        print('meds')\n",
    "        self.get_vs()\n",
    "        print('vs')\n",
    "        self.get_labs()\n",
    "        print('labs')\n",
    "        self.get_visits()\n",
    "        print('visit')\n",
    "        self.get_deaths()\n",
    "        print('deaths')\n",
    "        self.get_dx()\n",
    "        print('dx')\n",
    "        self.get_demo()\n",
    "        print('demo')\n",
    "        \n",
    "        self.ran_all = True\n",
    "\n",
    "    def merge(self):\n",
    "        if not self.ran_all:\n",
    "            raise Exception('Please run all first.')\n",
    "        \n",
    "        self.merged_df = (\n",
    "            self.visit_df\n",
    "            .join(self.vs_df, on=['ENC_HN', 'D001KEY'], how='outer_coalesce')\n",
    "            .join(self.dx_df, on=['ENC_HN', 'D001KEY'], how='outer_coalesce')\n",
    "            .join(self.meds_df, on=['ENC_HN', 'D001KEY'], how='outer_coalesce')\n",
    "            .join(self.lab_df, on=['ENC_HN', 'D001KEY'], how='outer_coalesce')\n",
    "            .join(self.deaths_df, on=['ENC_HN'], how='outer_coalesce')\n",
    "            .join(self.demo_df, on=['ENC_HN'], how='left')\n",
    "            .unique()\n",
    "        )\n",
    "\n",
    "        # Final Column Rename\n",
    "\n",
    "        self.merged_df = self.merged_df.rename({'D001KEY': 'Date', 'D035KEY': 'ICD10', 'D108KEY': 'Site'})\n",
    "    \n",
    "    def export(self) -> None:\n",
    "        if not self.ran_all:\n",
    "            raise Exception('Please run all first.')\n",
    "        self.file_name = self.export_folder / f'AF_warehouse_exported_{datetime.now().strftime('%d-%m-%Y')}'\n",
    "        print(f'Exporting {self.file_name}')\n",
    "        print(f'n = {self.merged_df['ENC_HN'].n_unique()}')\n",
    "        self.merged_df.write_parquet(self.file_name.with_suffix('.parquet.gzip'), compression='gzip')\n",
    "        print('Exported parquet')\n",
    "        self.merged_df.write_csv(self.file_name.with_suffix('.csv'))\n",
    "        print('Exported csv')\n",
    "        print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readme not included.\n",
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meds\n",
      "vs\n",
      "labs\n",
      "visit\n",
      "deaths\n",
      "dx\n",
      "demo\n",
      "Exporting ..\\output\\Dec23\\wh\\complete\\AF_warehouse_exported_22-02-2024\n",
      "n = 23337\n",
      "Exported parquet\n",
      "Exported csv\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "s = AFWarehouse(hn_list=hn, folder='D:/Datalake/Data/20231231_fu_nc')\n",
    "s.run_all()\n",
    "s.merge()\n",
    "s.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 32)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ENC_HN</th><th>Date</th><th>Site</th><th>SBP</th><th>DBP</th><th>HR</th><th>WEIGHT</th><th>HEIGHT</th><th>BMI</th><th>ICD10</th><th>Warfarin_sodium_5_mg</th><th>Warfarin_sodium_3_mg</th><th>Dabigatran_etexilate_110_mg</th><th>Rivaroxaban_10_mg</th><th>Dabigatran_etexilate_75_mg</th><th>Dabigatran_etexilate_150_mg</th><th>Rivaroxaban_15_mg</th><th>Rivaroxaban_20_mg</th><th>Apixaban_5_mg</th><th>Edoxaban_tosilate_60_mg</th><th>Rivaroxaban_2.5_mg</th><th>glucose</th><th>Serum creatinine</th><th>LDL</th><th>Triglyceride</th><th>HDL</th><th>Urine creatinine</th><th>Death_date</th><th>DOB</th><th>Sex</th><th>Province_ID</th><th>Province_Thai</th></tr><tr><td>str</td><td>date</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>date</td><td>date</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;02B9A45D4615C1…</td><td>2010-03-05</td><td>&quot;OMD15&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;J459&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2017-01-18</td><td>1932-02-15</td><td>&quot;F&quot;</td><td>&quot;10&quot;</td><td>&quot;กทม&quot;</td></tr><tr><td>&quot;06DDB477F640CA…</td><td>2010-09-01</td><td>&quot;ORP01&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;M4316, M5416&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1961-10-07</td><td>&quot;F&quot;</td><td>&quot;10&quot;</td><td>&quot;กทม&quot;</td></tr><tr><td>&quot;14872CEC0215B4…</td><td>2010-04-27</td><td>&quot;OFM05&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;G939&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1942-01-23</td><td>&quot;M&quot;</td><td>&quot;94&quot;</td><td>&quot;ปัตตานี&quot;</td></tr><tr><td>&quot;182325FC7323B7…</td><td>2010-05-25</td><td>&quot;OER101&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;I251, I48, R50…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1940-08-04</td><td>&quot;M&quot;</td><td>&quot;10&quot;</td><td>&quot;กทม&quot;</td></tr><tr><td>&quot;1AB53044B55507…</td><td>2010-05-08</td><td>&quot;OFM05&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1963-08-17</td><td>&quot;F&quot;</td><td>&quot;10&quot;</td><td>&quot;กทม&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 32)\n",
       "┌────────────────┬────────────┬────────┬──────┬───┬────────────┬─────┬─────────────┬───────────────┐\n",
       "│ ENC_HN         ┆ Date       ┆ Site   ┆ SBP  ┆ … ┆ DOB        ┆ Sex ┆ Province_ID ┆ Province_Thai │\n",
       "│ ---            ┆ ---        ┆ ---    ┆ ---  ┆   ┆ ---        ┆ --- ┆ ---         ┆ ---           │\n",
       "│ str            ┆ date       ┆ str    ┆ f64  ┆   ┆ date       ┆ str ┆ str         ┆ str           │\n",
       "╞════════════════╪════════════╪════════╪══════╪═══╪════════════╪═════╪═════════════╪═══════════════╡\n",
       "│ 02B9A45D4615C1 ┆ 2010-03-05 ┆ OMD15  ┆ null ┆ … ┆ 1932-02-15 ┆ F   ┆ 10          ┆ กทม           │\n",
       "│ 58EA4669EAACE6 ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ AC8E…          ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ 06DDB477F640CA ┆ 2010-09-01 ┆ ORP01  ┆ null ┆ … ┆ 1961-10-07 ┆ F   ┆ 10          ┆ กทม           │\n",
       "│ A4C075A4E9FEE6 ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ 3354…          ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ 14872CEC0215B4 ┆ 2010-04-27 ┆ OFM05  ┆ null ┆ … ┆ 1942-01-23 ┆ M   ┆ 94          ┆ ปัตตานี         │\n",
       "│ A6680B66FAB6C5 ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ 8C6A…          ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ 182325FC7323B7 ┆ 2010-05-25 ┆ OER101 ┆ null ┆ … ┆ 1940-08-04 ┆ M   ┆ 10          ┆ กทม           │\n",
       "│ FE7E3DB373BBD2 ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ F54F…          ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ 1AB53044B55507 ┆ 2010-05-08 ┆ OFM05  ┆ null ┆ … ┆ 1963-08-17 ┆ F   ┆ 10          ┆ กทม           │\n",
       "│ ACB09F0A2E45DD ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "│ 8A10…          ┆            ┆        ┆      ┆   ┆            ┆     ┆             ┆               │\n",
       "└────────────────┴────────────┴────────┴──────┴───┴────────────┴─────┴─────────────┴───────────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
